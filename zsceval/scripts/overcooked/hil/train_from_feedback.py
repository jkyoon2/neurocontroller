"""
Train Agents with Human Feedback (Offline HIL)

This script implements TRUE offline HIL training:
1. Loads agent checkpoints
2. Loads pre-saved buffers (NO environment interaction!)
3. Applies human feedback from JSON to buffers
4. Trains for multiple epochs on fixed buffers
5. Saves updated checkpoints

Usage:
    python -m zsceval.scripts.overcooked.hil.train_from_feedback \
        --layout_name random3 \
        --checkpoint_dir results/Overcooked/random3/rmappo/sp/seed10/models \
        --buffer_episode_id 42 \
        --feedback_json human_interface/data/feedback_from_human/feedback_episode_42.json \
        --output_dir results/Overcooked/random3/rmappo/hil/iteration_1 \
        --hil_train_epochs 50
        
Or specify individual checkpoints:
    python -m zsceval.scripts.overcooked.hil.train_from_feedback \
        --layout_name random3 \
        --checkpoint_agent0 path/to/actor_agent0.pt \
        --checkpoint_agent1 path/to/actor_agent1.pt \
        --buffer_episode_id 42 \
        --feedback_json feedback.json \
        --output_dir results/hil_iteration_1

Note: Buffer files (buffer_episode_42_agent0.pt, buffer_episode_42_agent1.pt)
      must exist in --buffer_dir (default: human_interface/data/buffers_for_training/)
      These are generated by generate_trajectory.py
"""

import argparse
import sys
import os
from pathlib import Path
from typing import List
import torch
import numpy as np
from loguru import logger

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from zsceval.config import get_config
from zsceval.overcooked_config import get_overcooked_args
from zsceval.envs.env_wrappers import SubprocVecEnv, DummyVecEnv
from zsceval.envs.overcooked_new.Overcooked_Env import Overcooked
from zsceval.algorithms.r_mappo.algorithm.rMAPPOPolicy import R_MAPPOPolicy
from zsceval.runner.separated.hil_offline_runner import HilOfflineRunner


def make_train_env(all_args):
    """Create training environment."""
    def get_env_fn(rank):
        def init_env():
            env = Overcooked(all_args=all_args, run_dir=".", rank=rank)
            return env
        return init_env
    
    if all_args.n_rollout_threads == 1:
        return DummyVecEnv([get_env_fn(0)])
    else:
        return SubprocVecEnv([get_env_fn(i) for i in range(all_args.n_rollout_threads)])


def find_latest_checkpoint(checkpoint_dir: Path, agent_id: int) -> Path:
    """Find the latest checkpoint for an agent."""
    # Look for periodic checkpoints first
    periodic_files = sorted(
        checkpoint_dir.glob(f"actor_agent{agent_id}_periodic_*.pt"),
        key=lambda p: int(p.stem.split('_')[-1])
    )
    if periodic_files:
        return periodic_files[-1]
    
    # Fallback to non-periodic
    regular_file = checkpoint_dir / f"actor_agent{agent_id}.pt"
    if regular_file.exists():
        return regular_file
    
    raise FileNotFoundError(f"No checkpoint found for agent {agent_id} in {checkpoint_dir}")


def parse_args():
    """Parse command line arguments."""
    parser = get_config()
    parser = get_overcooked_args(parser)
    
    # Checkpoint arguments
    checkpoint_group = parser.add_mutually_exclusive_group(required=True)
    checkpoint_group.add_argument(
        "--checkpoint_dir",
        type=str,
        help="Directory containing checkpoints to load (auto-finds latest)"
    )
    checkpoint_group.add_argument(
        "--checkpoint_agent0",
        type=str,
        help="Path to agent 0 checkpoint (use with --checkpoint_agent1)"
    )
    
    parser.add_argument(
        "--checkpoint_agent1",
        type=str,
        help="Path to agent 1 checkpoint (use with --checkpoint_agent0)"
    )
    
    # Buffer and Feedback arguments
    parser.add_argument(
        "--buffer_episode_id",
        type=int,
        help="Episode ID to load buffers for (auto-finds buffer files)"
    )
    parser.add_argument(
        "--buffer_dir",
        type=str,
        default="human_interface/data/buffers_for_training",
        help="Directory containing buffer files"
    )
    parser.add_argument(
        "--feedback_json",
        type=str,
        required=True,
        help="Path to feedback JSON file from human"
    )
    
    # Output argument
    parser.add_argument(
        "--output_dir",
        type=str,
        required=True,
        help="Output directory for updated checkpoints"
    )
    
    # HIL training arguments
    parser.add_argument(
        "--hil_train_epochs",
        type=int,
        default=10,
        help="Number of training epochs on fixed buffer"
    )
    parser.add_argument(
        "--num_replay_episodes",
        type=int,
        default=1,
        help="Number of episodes to replay for filling buffer"
    )
    parser.add_argument(
        "--hil_penalty_magnitude",
        type=float,
        default=-1.0,
        help="Reward penalty for error actions (M1)"
    )
    parser.add_argument(
        "--hil_constraint_coef",
        type=float,
        default=0.1,
        help="Policy constraint coefficient (M2)"
    )
    parser.add_argument(
        "--hil_enable_m2",
        action="store_true",
        default=True,
        help="Enable M2 (Policy Constraint)"
    )
    parser.add_argument(
        "--hil_enable_m3",
        action="store_true",
        default=False,
        help="Enable M3 (State Augmentation)"
    )
    
    args = parser.parse_args()
    
    # Add missing attributes for Overcooked environment
    if not hasattr(args, 'use_phi'):
        args.use_phi = False
    if not hasattr(args, 'old_dynamics'):
        args.old_dynamics = False
    
    # Validate checkpoint arguments
    if args.checkpoint_agent0 and not args.checkpoint_agent1:
        parser.error("--checkpoint_agent1 is required when using --checkpoint_agent0")
    if args.checkpoint_agent1 and not args.checkpoint_agent0:
        parser.error("--checkpoint_agent0 is required when using --checkpoint_agent1")
    
    # Validate feedback file exists
    if not Path(args.feedback_json).exists():
        parser.error(f"Feedback file not found: {args.feedback_json}")
    
    # Validate buffer_episode_id is provided
    if not args.buffer_episode_id and not args.checkpoint_dir:
        parser.error("--buffer_episode_id is required to locate buffer files")
    
    return args


def find_buffer_files(buffer_dir: Path, episode_id: int, num_agents: int = 2) -> List[Path]:
    """Find buffer files for a given episode.
    
    Args:
        buffer_dir: Directory containing buffers
        episode_id: Episode ID
        num_agents: Number of agents
        
    Returns:
        List of buffer file paths for each agent
    """
    buffer_paths = []
    for agent_id in range(num_agents):
        buffer_file = buffer_dir / f"buffer_episode_{episode_id}_agent{agent_id}.pt"
        if not buffer_file.exists():
            raise FileNotFoundError(f"Buffer file not found: {buffer_file}")
        buffer_paths.append(buffer_file)
    
    return buffer_paths


def main():
    """Main execution function."""
    args = parse_args()
    
    print("=" * 80)
    print("Offline HIL Training from Human Feedback")
    print("=" * 80)
    print(f"Layout: {args.layout_name}")
    print(f"Episode ID: {args.buffer_episode_id}")
    print(f"Feedback: {args.feedback_json}")
    print(f"Output: {args.output_dir}")
    print(f"Training epochs: {args.hil_train_epochs}")
    print("=" * 80)
    print()
    
    # Determine checkpoint paths
    if args.checkpoint_dir:
        checkpoint_dir = Path(args.checkpoint_dir)
        print(f"Finding latest checkpoints in: {checkpoint_dir}")
        checkpoint_paths = [
            find_latest_checkpoint(checkpoint_dir, 0),
            find_latest_checkpoint(checkpoint_dir, 1)
        ]
        print(f"  Agent 0: {checkpoint_paths[0].name}")
        print(f"  Agent 1: {checkpoint_paths[1].name}")
        
        # Don't set model_dir - we manually load checkpoints after runner creation
        args.model_dir = None
    else:
        checkpoint_paths = [
            Path(args.checkpoint_agent0),
            Path(args.checkpoint_agent1)
        ]
        print(f"Agent 0 checkpoint: {checkpoint_paths[0]}")
        print(f"Agent 1 checkpoint: {checkpoint_paths[1]}")
        
        # Don't set model_dir - we manually load checkpoints after runner creation
        args.model_dir = None
    
    print()
    
    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"✓ Output directory: {output_dir}")
    print()
    
    # Find buffer files
    print("Finding buffer files...")
    buffer_dir = Path(args.buffer_dir)
    buffer_paths = find_buffer_files(buffer_dir, args.buffer_episode_id, args.num_agents)
    print(f"✓ Found {len(buffer_paths)} buffer file(s):")
    for i, path in enumerate(buffer_paths):
        print(f"  - Agent {i}: {path.name}")
    print()
    
    # IMPORTANT: Offline HIL uses single episode buffer
    # Force n_rollout_threads=1
    if not hasattr(args, 'n_rollout_threads'):
        args.n_rollout_threads = 1
    elif args.n_rollout_threads != 1:
        logger.warning(
            f"Offline HIL requires n_rollout_threads=1 (got {args.n_rollout_threads}). "
            f"Forcing to 1."
        )
        args.n_rollout_threads = 1
    
    # Seed for reproducibility
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed_all(args.seed)
    np.random.seed(args.seed)
    
    # Initialize wandb if enabled
    use_wandb = getattr(args, 'use_wandb', False)
    print(f"Wandb {'enabled' if use_wandb else 'disabled'}")
    
    if use_wandb:
        import wandb
        run_name = f"hil_offline_{args.layout_name}_ep{args.buffer_episode_id}"
        
        # wandb_name in args is the entity name (e.g., "kyungyoon")
        # NOT the literal string "wandb_name"
        wandb_entity = getattr(args, 'wandb_name', None)
        
        wandb.init(
            project=args.env_name,
            entity=wandb_entity,  # Use actual entity name (e.g., "kyungyoon"), or None for default
            name=run_name,
            config=vars(args),
            dir=str(output_dir),
            group=f"hil_offline_{args.layout_name}",
            tags=["hil", "offline", args.layout_name]
        )
        print(f"✓ Wandb initialized: {run_name} (entity={wandb_entity})")
    else:
        print("Wandb disabled")
    
    # Device
    device = torch.device("cuda" if args.cuda and torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    print()
    
    # Create environment (single env for policy initialization only)
    # Note: Offline HIL doesn't interact with env, but needs spaces for policy init
    print("Creating environment (for policy initialization only)...")
    envs = make_train_env(args)
    print(f"✓ Environment created (n_rollout_threads=1)")
    print("  Note: Offline training doesn't interact with environment")
    print()
    
    # Create policies
    print("Initializing policies...")
    policy_list = []
    for agent_id in range(args.num_agents):
        policy = R_MAPPOPolicy(
            args,
            envs.observation_space[agent_id],
            envs.share_observation_space[agent_id],
            envs.action_space[agent_id],
            device=device
        )
        policy_list.append(policy)
    print(f"✓ {len(policy_list)} policies initialized")
    print()
    
    # Create config for runner
    config = {
        "all_args": args,
        "envs": envs,
        "eval_envs": None,
        "device": device,
        "num_agents": args.num_agents,
        "run_dir": output_dir,
        "save_dir": output_dir,  # Save directly to output_dir without models/ subdir
    }
    
    # Create HilOfflineRunner
    print("Creating HilOfflineRunner...")
    runner = HilOfflineRunner(config)
    print("✓ Runner created")
    print()
    
    # Load checkpoints manually (after runner has self.trainer)
    print("Loading checkpoints...")
    runner.load_checkpoints_from_paths(checkpoint_paths)
    print()
    
    # Run offline HIL training
    print("Starting offline HIL training...")
    print()
    
    all_epoch_infos = runner.run_offline_hil(
        buffer_paths=[str(p) for p in buffer_paths],
        feedback_json_path=args.feedback_json
    )
    
    # Close environment
    envs.close()
    
    print()
    print("=" * 80)
    print("✓ Offline HIL Training Complete!")
    print("=" * 80)
    print(f"Updated checkpoints saved to: {output_dir}")
    print()
    print("Next steps:")
    print("1. Generate new trajectories with updated checkpoints:")
    print(f"   python -m zsceval.scripts.overcooked.hil.generate_trajectory \\")
    print(f"       --checkpoint_dir {output_dir} \\")
    print(f"       --layout_name {args.layout_name}")
    print("2. Repeat the HIL cycle")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

